{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "parliamentary-triangle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten , Convolution2D, MaxPooling2D , Lambda, Conv2D, Activation,Concatenate, Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam , SGD , Adagrad\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers , initializers, activations\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump, load\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "import os \n",
    "import sys\n",
    "import time\n",
    "import importlib\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "importlib.reload(logging)\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "\n",
    "# limit GPU memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   # Restrict TensorFlow to only use the first GPU\n",
    "try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "    gpus[0],\n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10000)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "except RuntimeError as e:\n",
    "# Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-public",
   "metadata": {},
   "source": [
    "Ref: https://agustinus.kristia.de/techblog/2016/12/17/conditional-vae/   \n",
    "Ref: https://keras.io/guides/customizing_what_happens_in_fit/   \n",
    "Ref: https://keras.io/examples/generative/vae/   \n",
    "Ref: https://github.com/hagabbar/VItamin/blob/c1ae6dfa27b8ab77193caacddd477fde0dece1c2/Models/VICI_inverse_model.py#L404   \n",
    "Ref: https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalDiag    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "standing-election",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 7s, sys: 10.9 s, total: 3min 18s\n",
      "Wall time: 3min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_data = np.load(\"./nsi_data/sample_nsi_regression_1e7_v1.npz\")\n",
    "data_all = np.column_stack([training_data['ve_dune'][:,:36], training_data['vu_dune'][:,:36], training_data['vebar_dune'][:,:36], training_data['vubar_dune'][:,:36]])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_all)\n",
    "\n",
    "target = np.column_stack([training_data[\"theta23\"], training_data[\"delta\"]/180*np.pi ])\n",
    "\n",
    "x_train = data_all[:9000000]\n",
    "y_train = target[:9000000]\n",
    "y_train_angle = np.column_stack([y_train[:,0], np.sin(y_train[:,1]), np.cos(y_train[:,1])]) \n",
    "\n",
    "x_test = data_all[9000000:]\n",
    "y_test = target[9000000:]\n",
    "y_test_angle = np.column_stack([y_test[:,0], np.sin(y_test[:,1]), np.cos(y_test[:,1])]) \n",
    "\n",
    "x_train_poisson = np.random.poisson(x_train)/1000\n",
    "x_test_poisson = np.random.poisson(x_test)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lucky-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "particular-freedom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_parameter_inputs (Input [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_spectrum_inputs (InputL [(None, 144)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_parameter_1 (Dense)       (None, 64)           256         encoder_parameter_inputs[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_1 (Dense)        (None, 64)           9280        encoder_spectrum_inputs[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_parameter_2 (Dense)       (None, 32)           2080        dense_parameter_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_2 (Dense)        (None, 32)           2080        dense_spectrum_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_parameter_3 (Dense)       (None, 16)           528         dense_parameter_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_3 (Dense)        (None, 16)           528         dense_spectrum_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32)           0           dense_parameter_3[0][0]          \n",
      "                                                                 dense_spectrum_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 7)            231         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 7)            231         concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 15,214\n",
      "Trainable params: 15,214\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 7\n",
    "\n",
    "\"\"\"\n",
    "Encoder 1 (parameter + spectrum)\n",
    "\"\"\"\n",
    "# parameter\n",
    "encoder_parameter_inputs = layers.Input(shape=(3,),name = 'encoder_parameter_inputs')\n",
    "x_parameter = layers.Dense(64, activation=\"relu\", name = 'dense_parameter_1')(encoder_parameter_inputs)\n",
    "x_parameter = layers.Dense(32, activation=\"relu\", name = 'dense_parameter_2')(x_parameter)\n",
    "x_parameter = layers.Dense(16, activation=\"relu\", name = 'dense_parameter_3')(x_parameter)\n",
    "\n",
    "# spectrum\n",
    "encoder_spectrum_inputs = layers.Input(shape=(144),name = 'encoder_spectrum_inputs')\n",
    "x_spectrum = layers.Dense(64, activation=\"relu\", name = 'dense_spectrum_1')(encoder_spectrum_inputs)\n",
    "x_spectrum = layers.Dense(32, activation=\"relu\", name = 'dense_spectrum_2')(x_spectrum)\n",
    "x_spectrum = layers.Dense(16, activation=\"relu\", name = 'dense_spectrum_3')(x_spectrum)\n",
    "\n",
    "# merged\n",
    "mergedOut_Encoder_1 = Concatenate()([x_parameter,x_spectrum])\n",
    "\n",
    "# sampling\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(mergedOut_Encoder_1)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(mergedOut_Encoder_1)\n",
    "# z = Sampling(name = 'Sampling_encoder')([z_mean, z_log_var])\n",
    "\n",
    "# build model\n",
    "encoder_1 = keras.Model([encoder_parameter_inputs, encoder_spectrum_inputs], [z_mean, z_log_var], name=\"encoder_1\")\n",
    "encoder_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "controlled-photographer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_spectrum_inputs (InputL [(None, 144)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_1 (Dense)        (None, 64)           9280        encoder_spectrum_inputs[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_2 (Dense)        (None, 32)           2080        dense_spectrum_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_3 (Dense)        (None, 16)           528         dense_spectrum_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 210)          3570        dense_spectrum_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 210)          3570        dense_spectrum_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "z_weight (Dense)                (None, 30)           510         dense_spectrum_3[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 19,538\n",
      "Trainable params: 19,538\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Encoder 2 (spectrum)\n",
    "\"\"\"\n",
    "# spectrum\n",
    "encoder_spectrum_inputs = layers.Input(shape=(144,),name = 'encoder_spectrum_inputs')\n",
    "x_spectrum = layers.Dense(64, activation=\"relu\", name = 'dense_spectrum_1')(encoder_spectrum_inputs)\n",
    "x_spectrum = layers.Dense(32, activation=\"relu\", name = 'dense_spectrum_2')(x_spectrum)\n",
    "x_spectrum = layers.Dense(16, activation=\"relu\", name = 'dense_spectrum_3')(x_spectrum)\n",
    "\n",
    "# sampling\n",
    "guassian_number = 30\n",
    "z_mean = layers.Dense(guassian_number*latent_dim, name=\"z_mean\")(x_spectrum)\n",
    "z_log_var = layers.Dense(guassian_number*latent_dim, name=\"z_log_var\")(x_spectrum)\n",
    "z_weight = layers.Dense(guassian_number, name=\"z_weight\")(x_spectrum)\n",
    "# z = Sampling(name = 'Sampling_encoder')([z_mean, z_log_var])\n",
    "\n",
    "# build model\n",
    "encoder_2 = keras.Model(encoder_spectrum_inputs, [z_mean, z_log_var, z_weight], name=\"encoder_2\")\n",
    "encoder_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fantastic-terrorism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_latent_inputs (InputLay [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_spectrum_inputs (InputL [(None, 144)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           512         decoder_latent_inputs[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_1 (Dense)        (None, 64)           9280        decoder_spectrum_inputs[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_2 (Dense)        (None, 32)           2080        dense_spectrum_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           528         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_3 (Dense)        (None, 16)           528         dense_spectrum_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32)           0           dense_3[0][0]                    \n",
      "                                                                 dense_spectrum_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 3)            99          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 3)            99          concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 15,206\n",
      "Trainable params: 15,206\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Decoder Model (latent + spectrum)\n",
    "\"\"\"\n",
    "latent_dim_2 = 3\n",
    "\n",
    "decoder_latent_inputs = keras.Input(shape=(latent_dim,),name = 'decoder_latent_inputs')\n",
    "x_latent = layers.Dense(64, activation=\"relu\", name = 'dense_1')(decoder_latent_inputs)\n",
    "x_latent = layers.Dense(32, activation=\"relu\", name = 'dense_2')(x_latent)\n",
    "x_latent = layers.Dense(16, activation=\"relu\", name = 'dense_3')(x_latent)\n",
    "\n",
    "# spectrum\n",
    "decoder_spectrum_inputs = layers.Input(shape=(144,),name = 'decoder_spectrum_inputs')\n",
    "x_spectrum = layers.Dense(64, activation=\"relu\", name = 'dense_spectrum_1')(decoder_spectrum_inputs)\n",
    "x_spectrum = layers.Dense(32, activation=\"relu\", name = 'dense_spectrum_2')(x_spectrum)\n",
    "x_spectrum = layers.Dense(16, activation=\"relu\", name = 'dense_spectrum_3')(x_spectrum)\n",
    "\n",
    "# merged\n",
    "mergedOut_Decoder = Concatenate()([x_latent,x_spectrum])\n",
    "\n",
    "z2_mean = layers.Dense(latent_dim_2, name=\"z_mean\")(mergedOut_Decoder)\n",
    "z2_log_var = layers.Dense(latent_dim_2, name=\"z_log_var\")(mergedOut_Decoder)\n",
    "# z2 = Sampling(name = 'Sampling_decoder')([z2_mean, z2_log_var])\n",
    "\n",
    "decoder = keras.Model([decoder_latent_inputs, decoder_spectrum_inputs], [z2_mean, z2_log_var], name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "specific-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref: https://keras.io/guides/customizing_what_happens_in_fit/\n",
    "#Ref: https://keras.io/examples/generative/vae/\n",
    "#Ref: https://github.com/hagabbar/VItamin/blob/c1ae6dfa27b8ab77193caacddd477fde0dece1c2/Models/VICI_inverse_model.py#L404\n",
    "class CVAE(keras.Model):\n",
    "    def __init__(self, encoder1, encoder2, decoder, **kwargs):\n",
    "        super(CVAE, self).__init__(**kwargs)\n",
    "        self.encoder1 = encoder1  #(parameter + spectrum)\n",
    "        self.encoder2 = encoder2  #(spectrum)\n",
    "        self.decoder = decoder    #(latent + spectrum)\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "                ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            SMALL_CONSTANT = 1e-12 # necessary to prevent the division by zero in many operations \n",
    "            \n",
    "            # encoder1: \n",
    "            # input: [parameter, spectrum]\n",
    "            # output: [mean, log_var]\n",
    "            # mean.shape= (N, 15)  log_var.shape= (N, 15)   D(latent space) = 15\n",
    "            z1_mean, z1_log_var = self.encoder1(x)  #(parameter + spectrum)\n",
    "            \n",
    "            # GET q(z|x,y)   #(parameter + spectrum)\n",
    "            temp_var_q = SMALL_CONSTANT + tf.exp(z1_log_var)\n",
    "            mvn_q = tfp.distributions.MultivariateNormalDiag(\n",
    "                          loc=z1_mean,\n",
    "                          scale_diag=tf.sqrt(temp_var_q))\n",
    "            \n",
    "            z1 = mvn_q.sample()\n",
    "            \n",
    "            # encoder2: \n",
    "            # input: spectrum\n",
    "            # output: [mean, log_var, weight]\n",
    "            # mean.shape= (N, 10*15)  log_var.shape= (N, 10*15) weight.shape= (N, 10)  \n",
    "            # D(latent space)=15, D(mixture component)=10\n",
    "            z2_mean, z2_log_var, z2_weight = self.encoder2(x[1])  #(spectrum)\n",
    "\n",
    "            # mean->reshape= (N, 10, 15  log_var->reshape= (N, 10, 15) for create mixture model\n",
    "            z2_mean = tf.reshape(z2_mean, (-1, guassian_number, 7))\n",
    "            z2_log_var = tf.reshape(z2_log_var, (-1, guassian_number, 7))\n",
    "            z2_weight = tf.reshape(z2_weight, (-1, guassian_number))\n",
    "\n",
    "            # Get r1(z|y) mixture model   #(spectrum)\n",
    "            temp_var_r1 = SMALL_CONSTANT + tf.exp(z2_log_var)\n",
    "            bimix_gauss = tfp.distributions.MixtureSameFamily(\n",
    "                          mixture_distribution=tfp.distributions.Categorical(logits=z2_weight),\n",
    "                          components_distribution=tfp.distributions.MultivariateNormalDiag(\n",
    "                          loc=z2_mean,\n",
    "                          scale_diag=tf.sqrt(temp_var_r1)))\n",
    "            \n",
    "            z2 = bimix_gauss.sample()\n",
    "            \n",
    "            # decoder: \n",
    "            # input: [latent, spectrum]\n",
    "            # output: [mean, log_var]\n",
    "            # mean.shape= (N, 2)  log_var.shape= (N, 2) \n",
    "            reconstruction_mean, reconstruction_var = self.decoder([z1, x[1]])      #(latent + spectrum)\n",
    "            \n",
    "            # GET r2(x|z,y)    #(latent + spectrum)\n",
    "            temp_var_r2 = SMALL_CONSTANT + tf.exp(reconstruction_var)\n",
    "            reconstruction_parameter = tfp.distributions.MultivariateNormalDiag(\n",
    "                                     loc=reconstruction_mean,\n",
    "                                     scale_diag= tf.sqrt(temp_var_r2))\n",
    "            \n",
    "            r2 = reconstruction_parameter.sample()\n",
    "        \n",
    "            log_q_q = mvn_q.log_prob(z1)\n",
    "            log_r1_q = bimix_gauss.log_prob(z1)               # evaluate the log prob of r1 at the q samples(z1)\n",
    "            kl_loss = tf.reduce_mean(log_q_q - log_r1_q)      # average over batch\n",
    "            \n",
    "            reconstruction_parameter_loss = reconstruction_parameter.log_prob(y)\n",
    "            reconstruction_loss = -1.0*tf.reduce_mean(reconstruction_parameter_loss)\n",
    "            \n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "subsequent-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Training\n",
    "\"\"\"\n",
    "cvae = CVAE(encoder_1,encoder_2,decoder)\n",
    "cvae.compile(optimizer=keras.optimizers.Adam())\n",
    "# vae.compile(optimizer=keras.optimizers.Adadelta())\n",
    "\n",
    "check_list=[]\n",
    "csv_logger = CSVLogger(\"./Training_loss/CVAE_1DCNN_training_log_v2.csv\")\n",
    "check_list.append(csv_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sound-throw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/distributions/distribution.py:298: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it with `scale_diag` directly instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/linalg/linear_operator_diag.py:175: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 41886.9641 - reconstruction_loss: 4334.1528 - kl_loss: 0.5930\n",
      "Epoch 2/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 4.2723 - reconstruction_loss: 3.4686 - kl_loss: 0.5687\n",
      "Epoch 3/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 3.3063 - reconstruction_loss: 1.5726 - kl_loss: 1.4927\n",
      "Epoch 4/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 2.4949 - reconstruction_loss: 0.9536 - kl_loss: 1.4118\n",
      "Epoch 5/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 2.0463 - reconstruction_loss: 0.4263 - kl_loss: 1.5340\n",
      "Epoch 6/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 1.7418 - reconstruction_loss: 0.0678 - kl_loss: 1.6152\n",
      "Epoch 7/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 1.5408 - reconstruction_loss: -0.1481 - kl_loss: 1.6541\n",
      "Epoch 8/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 1.4013 - reconstruction_loss: -0.3232 - kl_loss: 1.6977\n",
      "Epoch 9/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 1.2999 - reconstruction_loss: -0.4217 - kl_loss: 1.7079\n",
      "Epoch 10/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 1.2140 - reconstruction_loss: -0.5421 - kl_loss: 1.7307\n",
      "Epoch 11/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 1.1343 - reconstruction_loss: -0.6064 - kl_loss: 1.7242\n",
      "Epoch 12/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 1.0695 - reconstruction_loss: -0.6426 - kl_loss: 1.7040\n",
      "Epoch 13/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 1.0114 - reconstruction_loss: -0.6877 - kl_loss: 1.6945\n",
      "Epoch 14/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.9832 - reconstruction_loss: -0.7108 - kl_loss: 1.6818\n",
      "Epoch 15/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.9368 - reconstruction_loss: -0.7533 - kl_loss: 1.6866\n",
      "Epoch 16/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.8710 - reconstruction_loss: -0.8803 - kl_loss: 1.7268\n",
      "Epoch 17/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.7781 - reconstruction_loss: -0.9459 - kl_loss: 1.7109\n",
      "Epoch 18/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.7160 - reconstruction_loss: -0.9985 - kl_loss: 1.7054\n",
      "Epoch 19/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.6801 - reconstruction_loss: -1.0508 - kl_loss: 1.7004\n",
      "Epoch 20/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.6180 - reconstruction_loss: -1.0908 - kl_loss: 1.7005\n",
      "Epoch 21/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.6376 - reconstruction_loss: -1.0895 - kl_loss: 1.6859\n",
      "Epoch 22/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.5590 - reconstruction_loss: -1.1611 - kl_loss: 1.6996\n",
      "Epoch 23/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.5279 - reconstruction_loss: -1.1860 - kl_loss: 1.7048\n",
      "Epoch 24/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.6217 - reconstruction_loss: -0.9312 - kl_loss: 1.7176\n",
      "Epoch 25/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.5182 - reconstruction_loss: -1.2687 - kl_loss: 1.7460\n",
      "Epoch 26/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.4511 - reconstruction_loss: -1.2506 - kl_loss: 1.7116\n",
      "Epoch 27/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.4314 - reconstruction_loss: -1.2822 - kl_loss: 1.7126\n",
      "Epoch 28/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.4103 - reconstruction_loss: -1.3106 - kl_loss: 1.7120\n",
      "Epoch 29/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.3939 - reconstruction_loss: -1.3258 - kl_loss: 1.7219\n",
      "Epoch 30/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.3711 - reconstruction_loss: -1.3631 - kl_loss: 1.7255\n",
      "Epoch 31/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.3791 - reconstruction_loss: -1.3401 - kl_loss: 1.7156\n",
      "Epoch 32/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.5084 - reconstruction_loss: -1.2929 - kl_loss: 1.7481\n",
      "Epoch 33/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.3243 - reconstruction_loss: -1.4191 - kl_loss: 1.7384\n",
      "Epoch 34/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.3190 - reconstruction_loss: -1.4116 - kl_loss: 1.7287\n",
      "Epoch 35/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.2847 - reconstruction_loss: -1.4453 - kl_loss: 1.7348\n",
      "Epoch 36/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.2886 - reconstruction_loss: -1.4401 - kl_loss: 1.7334\n",
      "Epoch 37/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.3499 - reconstruction_loss: -1.4369 - kl_loss: 1.7579\n",
      "Epoch 38/300\n",
      "9000/9000 [==============================] - 60s 7ms/step - loss: 0.2603 - reconstruction_loss: -1.4984 - kl_loss: 1.7538\n",
      "Epoch 39/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.2357 - reconstruction_loss: -1.3480 - kl_loss: 1.7494\n",
      "Epoch 40/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.3545 - reconstruction_loss: -1.5179 - kl_loss: 1.7948\n",
      "Epoch 41/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.2328 - reconstruction_loss: -1.5607 - kl_loss: 1.7954\n",
      "Epoch 42/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.2213 - reconstruction_loss: -1.5750 - kl_loss: 1.7971\n",
      "Epoch 43/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.2069 - reconstruction_loss: -1.5929 - kl_loss: 1.8027\n",
      "Epoch 44/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1967 - reconstruction_loss: -1.5458 - kl_loss: 1.8105\n",
      "Epoch 45/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.2424 - reconstruction_loss: -1.5911 - kl_loss: 1.8139\n",
      "Epoch 46/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1682 - reconstruction_loss: -1.6173 - kl_loss: 1.7917\n",
      "Epoch 47/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1993 - reconstruction_loss: -1.6039 - kl_loss: 1.7963\n",
      "Epoch 48/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1894 - reconstruction_loss: -1.6244 - kl_loss: 1.8125\n",
      "Epoch 49/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1503 - reconstruction_loss: -1.6433 - kl_loss: 1.8007\n",
      "Epoch 50/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1562 - reconstruction_loss: -1.6384 - kl_loss: 1.7940\n",
      "Epoch 51/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1534 - reconstruction_loss: -1.6764 - kl_loss: 1.8283\n",
      "Epoch 52/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1468 - reconstruction_loss: -1.6550 - kl_loss: 1.8291\n",
      "Epoch 53/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1503 - reconstruction_loss: -1.6875 - kl_loss: 1.8243\n",
      "Epoch 54/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.2024 - reconstruction_loss: -1.5346 - kl_loss: 1.8170\n",
      "Epoch 55/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1409 - reconstruction_loss: -1.5830 - kl_loss: 1.8335\n",
      "Epoch 56/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: 0.1175 - reconstruction_loss: -1.6856 - kl_loss: 1.8184\n",
      "Epoch 57/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.1202 - reconstruction_loss: -1.7047 - kl_loss: 1.8203\n",
      "Epoch 58/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.0933 - reconstruction_loss: -1.7169 - kl_loss: 1.8234\n",
      "Epoch 59/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.0974 - reconstruction_loss: -1.6947 - kl_loss: 1.8236\n",
      "Epoch 60/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 0.0269 - reconstruction_loss: -1.7290 - kl_loss: 1.8161\n",
      "Epoch 61/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.0107 - reconstruction_loss: -1.8020 - kl_loss: 1.7790\n",
      "Epoch 62/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.0411 - reconstruction_loss: -1.8271 - kl_loss: 1.7922\n",
      "Epoch 63/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.0500 - reconstruction_loss: -1.8698 - kl_loss: 1.8090\n",
      "Epoch 64/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.0622 - reconstruction_loss: -1.8724 - kl_loss: 1.8188\n",
      "Epoch 65/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.0317 - reconstruction_loss: -1.8553 - kl_loss: 1.8052\n",
      "Epoch 66/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.0712 - reconstruction_loss: -1.8954 - kl_loss: 1.8181\n",
      "Epoch 67/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.0685 - reconstruction_loss: -1.9035 - kl_loss: 1.8306\n",
      "Epoch 68/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1209 - reconstruction_loss: -1.9254 - kl_loss: 1.8342\n",
      "Epoch 69/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1341 - reconstruction_loss: -1.9758 - kl_loss: 1.8486\n",
      "Epoch 70/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1217 - reconstruction_loss: -1.9588 - kl_loss: 1.8431\n",
      "Epoch 71/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.1129 - reconstruction_loss: -1.9763 - kl_loss: 1.8568\n",
      "Epoch 72/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1429 - reconstruction_loss: -2.0054 - kl_loss: 1.8679\n",
      "Epoch 73/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1454 - reconstruction_loss: -1.9915 - kl_loss: 1.8748\n",
      "Epoch 74/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1534 - reconstruction_loss: -1.9976 - kl_loss: 1.8724\n",
      "Epoch 75/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1700 - reconstruction_loss: -2.0537 - kl_loss: 1.9052\n",
      "Epoch 76/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1414 - reconstruction_loss: -2.0128 - kl_loss: 1.8737\n",
      "Epoch 77/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1418 - reconstruction_loss: -2.0241 - kl_loss: 1.8815\n",
      "Epoch 78/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1535 - reconstruction_loss: -2.0339 - kl_loss: 1.8778\n",
      "Epoch 79/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1532 - reconstruction_loss: -2.0248 - kl_loss: 1.8674\n",
      "Epoch 80/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.0722 - reconstruction_loss: -2.0236 - kl_loss: 1.9035\n",
      "Epoch 81/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.1715 - reconstruction_loss: -2.0298 - kl_loss: 1.8729\n",
      "Epoch 82/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1712 - reconstruction_loss: -2.0232 - kl_loss: 1.8666\n",
      "Epoch 83/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1846 - reconstruction_loss: -2.0420 - kl_loss: 1.8731\n",
      "Epoch 84/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1461 - reconstruction_loss: -2.0201 - kl_loss: 1.8789\n",
      "Epoch 85/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1577 - reconstruction_loss: -2.0144 - kl_loss: 1.8708\n",
      "Epoch 86/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1797 - reconstruction_loss: -2.0589 - kl_loss: 1.8743\n",
      "Epoch 87/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1955 - reconstruction_loss: -2.0120 - kl_loss: 1.8535\n",
      "Epoch 88/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2088 - reconstruction_loss: -2.0979 - kl_loss: 1.8933\n",
      "Epoch 89/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1585 - reconstruction_loss: -2.0388 - kl_loss: 1.8738\n",
      "Epoch 90/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1961 - reconstruction_loss: -2.0571 - kl_loss: 1.8611\n",
      "Epoch 91/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1915 - reconstruction_loss: -2.0544 - kl_loss: 1.8726\n",
      "Epoch 92/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.1910 - reconstruction_loss: -2.0530 - kl_loss: 1.8631\n",
      "Epoch 93/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1788 - reconstruction_loss: -2.0633 - kl_loss: 1.8706\n",
      "Epoch 94/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2014 - reconstruction_loss: -2.0762 - kl_loss: 1.8865\n",
      "Epoch 95/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1763 - reconstruction_loss: -2.0451 - kl_loss: 1.8723\n",
      "Epoch 96/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2308 - reconstruction_loss: -2.1184 - kl_loss: 1.8981\n",
      "Epoch 97/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2123 - reconstruction_loss: -2.0992 - kl_loss: 1.8802\n",
      "Epoch 98/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2238 - reconstruction_loss: -2.0854 - kl_loss: 1.8716\n",
      "Epoch 99/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.1873 - reconstruction_loss: -2.0690 - kl_loss: 1.8774\n",
      "Epoch 100/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2204 - reconstruction_loss: -2.0303 - kl_loss: 1.8179\n",
      "Epoch 101/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2244 - reconstruction_loss: -2.0801 - kl_loss: 1.8699\n",
      "Epoch 102/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2315 - reconstruction_loss: -2.0819 - kl_loss: 1.8652\n",
      "Epoch 103/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.2358 - reconstruction_loss: -2.0943 - kl_loss: 1.8598\n",
      "Epoch 104/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2306 - reconstruction_loss: -2.1297 - kl_loss: 1.8968\n",
      "Epoch 105/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.2295 - reconstruction_loss: -2.1282 - kl_loss: 1.8937\n",
      "Epoch 106/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2325 - reconstruction_loss: -2.0493 - kl_loss: 1.8367\n",
      "Epoch 107/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2212 - reconstruction_loss: -2.0953 - kl_loss: 1.8670\n",
      "Epoch 108/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2215 - reconstruction_loss: -2.0609 - kl_loss: 1.8336\n",
      "Epoch 109/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.2252 - reconstruction_loss: -2.1022 - kl_loss: 1.8736\n",
      "Epoch 110/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.2249 - reconstruction_loss: -2.0452 - kl_loss: 1.8295\n",
      "Epoch 111/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2364 - reconstruction_loss: -2.0012 - kl_loss: 1.7709\n",
      "Epoch 112/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.2393 - reconstruction_loss: -2.0376 - kl_loss: 1.8326\n",
      "Epoch 113/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2117 - reconstruction_loss: -2.0680 - kl_loss: 1.8342\n",
      "Epoch 114/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2511 - reconstruction_loss: -2.0496 - kl_loss: 1.8060\n",
      "Epoch 115/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2407 - reconstruction_loss: -2.0978 - kl_loss: 1.8439\n",
      "Epoch 116/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2471 - reconstruction_loss: -2.0500 - kl_loss: 1.8106\n",
      "Epoch 117/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2381 - reconstruction_loss: -2.0307 - kl_loss: 1.7995\n",
      "Epoch 120/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2510 - reconstruction_loss: -2.0704 - kl_loss: 1.8400\n",
      "Epoch 121/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.2484 - reconstruction_loss: -2.0301 - kl_loss: 1.7905\n",
      "Epoch 122/300\n",
      "1341/9000 [===>..........................] - ETA: 49s - loss: -0.2326 - reconstruction_loss: -2.0066 - kl_loss: 1.7820"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2466 - reconstruction_loss: -2.0601 - kl_loss: 1.8140\n",
      "Epoch 130/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2784 - reconstruction_loss: -2.0485 - kl_loss: 1.7759\n",
      "Epoch 133/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2703 - reconstruction_loss: -2.0602 - kl_loss: 1.7950\n",
      "Epoch 134/300\n",
      "7997/9000 [=========================>....] - ETA: 6s - loss: -0.2702 - reconstruction_loss: -2.0377 - kl_loss: 1.7625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.2989 - reconstruction_loss: -2.1119 - kl_loss: 1.8270\n",
      "Epoch 143/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.2988 - reconstruction_loss: -2.1375 - kl_loss: 1.8361\n",
      "Epoch 144/300\n",
      "8006/9000 [=========================>....] - ETA: 6s - loss: -0.2899 - reconstruction_loss: -2.0952 - kl_loss: 1.8142"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3028 - reconstruction_loss: -2.0652 - kl_loss: 1.7679\n",
      "Epoch 153/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3194 - reconstruction_loss: -2.1004 - kl_loss: 1.7981\n",
      "Epoch 154/300\n",
      "7574/9000 [========================>.....] - ETA: 9s - loss: -0.3071 - reconstruction_loss: -2.1548 - kl_loss: 1.8432"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3158 - reconstruction_loss: -2.1344 - kl_loss: 1.8211\n",
      "Epoch 162/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3084 - reconstruction_loss: -2.1671 - kl_loss: 1.8485\n",
      "Epoch 163/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3105 - reconstruction_loss: -2.1756 - kl_loss: 1.8551\n",
      "Epoch 164/300\n",
      "2645/9000 [=======>......................] - ETA: 40s - loss: -0.2891 - reconstruction_loss: -2.1442 - kl_loss: 1.8432"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3292 - reconstruction_loss: -2.1834 - kl_loss: 1.8550\n",
      "Epoch 172/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3596 - reconstruction_loss: -2.1933 - kl_loss: 1.8449\n",
      "Epoch 179/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3407 - reconstruction_loss: -2.1664 - kl_loss: 1.8321\n",
      "Epoch 180/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3648 - reconstruction_loss: -2.2183 - kl_loss: 1.8620\n",
      "Epoch 181/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3543 - reconstruction_loss: -2.1727 - kl_loss: 1.8201\n",
      "Epoch 182/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3480 - reconstruction_loss: -2.1577 - kl_loss: 1.8160\n",
      "Epoch 183/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3463 - reconstruction_loss: -2.1826 - kl_loss: 1.8451\n",
      "Epoch 184/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3417 - reconstruction_loss: -2.1151 - kl_loss: 1.7744\n",
      "Epoch 185/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3097 - reconstruction_loss: -2.1165 - kl_loss: 1.7988\n",
      "Epoch 186/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3584 - reconstruction_loss: -2.1758 - kl_loss: 1.8260\n",
      "Epoch 187/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3598 - reconstruction_loss: -2.1832 - kl_loss: 1.8273\n",
      "Epoch 188/300\n",
      "9000/9000 [==============================] - 57s 6ms/step - loss: -0.3633 - reconstruction_loss: -2.1914 - kl_loss: 1.8324\n",
      "Epoch 189/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3687 - reconstruction_loss: -2.1695 - kl_loss: 1.8132\n",
      "Epoch 190/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3517 - reconstruction_loss: -2.1819 - kl_loss: 1.8212\n",
      "Epoch 191/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3585 - reconstruction_loss: -2.1842 - kl_loss: 1.8409\n",
      "Epoch 192/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3712 - reconstruction_loss: -2.1741 - kl_loss: 1.8116\n",
      "Epoch 193/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3779 - reconstruction_loss: -2.2155 - kl_loss: 1.8502\n",
      "Epoch 194/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3740 - reconstruction_loss: -2.1999 - kl_loss: 1.8308\n",
      "Epoch 195/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3597 - reconstruction_loss: -2.1704 - kl_loss: 1.8059\n",
      "Epoch 196/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3581 - reconstruction_loss: -2.1531 - kl_loss: 1.7968\n",
      "Epoch 197/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3719 - reconstruction_loss: -2.1493 - kl_loss: 1.7899\n",
      "Epoch 198/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3474 - reconstruction_loss: -2.1497 - kl_loss: 1.7919\n",
      "Epoch 199/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3537 - reconstruction_loss: -2.1578 - kl_loss: 1.8014\n",
      "Epoch 200/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3231 - reconstruction_loss: -2.1319 - kl_loss: 1.7896\n",
      "Epoch 201/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3710 - reconstruction_loss: -2.1655 - kl_loss: 1.8051\n",
      "Epoch 202/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3629 - reconstruction_loss: -2.1028 - kl_loss: 1.7527\n",
      "Epoch 203/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3430 - reconstruction_loss: -2.1331 - kl_loss: 1.7739\n",
      "Epoch 204/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3724 - reconstruction_loss: -2.1769 - kl_loss: 1.8063\n",
      "Epoch 205/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3472 - reconstruction_loss: -2.1445 - kl_loss: 1.7948\n",
      "Epoch 206/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3797 - reconstruction_loss: -2.1770 - kl_loss: 1.8015\n",
      "Epoch 207/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3698 - reconstruction_loss: -2.1405 - kl_loss: 1.7685\n",
      "Epoch 208/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3754 - reconstruction_loss: -2.1727 - kl_loss: 1.8081\n",
      "Epoch 209/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3848 - reconstruction_loss: -2.1658 - kl_loss: 1.7894\n",
      "Epoch 210/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3842 - reconstruction_loss: -2.1766 - kl_loss: 1.8014\n",
      "Epoch 211/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3652 - reconstruction_loss: -2.1339 - kl_loss: 1.7862\n",
      "Epoch 212/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3925 - reconstruction_loss: -2.1854 - kl_loss: 1.8048\n",
      "Epoch 213/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3850 - reconstruction_loss: -2.1758 - kl_loss: 1.8009\n",
      "Epoch 214/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3727 - reconstruction_loss: -2.1076 - kl_loss: 1.7369\n",
      "Epoch 215/300\n",
      "  41/9000 [..............................] - ETA: 59s - loss: -0.4069 - reconstruction_loss: -2.2421 - kl_loss: 1.8093"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3749 - reconstruction_loss: -2.1667 - kl_loss: 1.7941\n",
      "Epoch 223/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3846 - reconstruction_loss: -2.2023 - kl_loss: 1.8127\n",
      "Epoch 224/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3837 - reconstruction_loss: -2.1815 - kl_loss: 1.7955\n",
      "Epoch 225/300\n",
      "1782/9000 [====>.........................] - ETA: 46s - loss: -0.3859 - reconstruction_loss: -2.1139 - kl_loss: 1.7434"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3898 - reconstruction_loss: -2.1387 - kl_loss: 1.7605\n",
      "Epoch 233/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3853 - reconstruction_loss: -2.1855 - kl_loss: 1.7951\n",
      "Epoch 234/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3484 - reconstruction_loss: -2.1733 - kl_loss: 1.8016\n",
      "Epoch 235/300\n",
      "3642/9000 [===========>..................] - ETA: 34s - loss: -0.4044 - reconstruction_loss: -2.1637 - kl_loss: 1.8020"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.4008 - reconstruction_loss: -2.1995 - kl_loss: 1.8094\n",
      "Epoch 243/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.4125 - reconstruction_loss: -2.2183 - kl_loss: 1.8102\n",
      "Epoch 244/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.3893 - reconstruction_loss: -2.2022 - kl_loss: 1.8087\n",
      "Epoch 245/300\n",
      "4088/9000 [============>.................] - ETA: 31s - loss: -0.4115 - reconstruction_loss: -2.2186 - kl_loss: 1.8201"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.4068 - reconstruction_loss: -2.2049 - kl_loss: 1.7986\n",
      "Epoch 253/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.4125 - reconstruction_loss: -2.2177 - kl_loss: 1.8036\n",
      "Epoch 282/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.4178 - reconstruction_loss: -2.2250 - kl_loss: 1.8117\n",
      "Epoch 283/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.4095 - reconstruction_loss: -2.1770 - kl_loss: 1.7723\n",
      "Epoch 284/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.4019 - reconstruction_loss: -2.2041 - kl_loss: 1.7966\n",
      "Epoch 285/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.4191 - reconstruction_loss: -2.2268 - kl_loss: 1.8091\n",
      "Epoch 286/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.4193 - reconstruction_loss: -2.2202 - kl_loss: 1.8001\n",
      "Epoch 287/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.4034 - reconstruction_loss: -2.2346 - kl_loss: 1.8164\n",
      "Epoch 288/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.4242 - reconstruction_loss: -2.2287 - kl_loss: 1.8081\n",
      "Epoch 289/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.4225 - reconstruction_loss: -2.2056 - kl_loss: 1.7967\n",
      "Epoch 290/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.4096 - reconstruction_loss: -2.1652 - kl_loss: 1.7690\n",
      "Epoch 291/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.4343 - reconstruction_loss: -2.2328 - kl_loss: 1.8042\n",
      "Epoch 292/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.3525 - reconstruction_loss: -2.2116 - kl_loss: 1.8254\n",
      "Epoch 293/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.4190 - reconstruction_loss: -2.2205 - kl_loss: 1.8066\n",
      "Epoch 294/300\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: -0.4069 - reconstruction_loss: -2.2140 - kl_loss: 1.8011\n",
      "Epoch 295/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.4147 - reconstruction_loss: -2.2359 - kl_loss: 1.8179\n",
      "Epoch 296/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.4123 - reconstruction_loss: -2.1978 - kl_loss: 1.7844\n",
      "Epoch 297/300\n",
      "9000/9000 [==============================] - 57s 6ms/step - loss: -0.4197 - reconstruction_loss: -2.2272 - kl_loss: 1.8067\n",
      "Epoch 298/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.4224 - reconstruction_loss: -2.2461 - kl_loss: 1.8200\n",
      "Epoch 299/300\n",
      "9000/9000 [==============================] - 57s 6ms/step - loss: -0.4232 - reconstruction_loss: -2.2197 - kl_loss: 1.7981\n",
      "Epoch 300/300\n",
      "9000/9000 [==============================] - 58s 6ms/step - loss: -0.4203 - reconstruction_loss: -2.2323 - kl_loss: 1.8173\n",
      "CPU times: user 6h 14min 20s, sys: 32min 7s, total: 6h 46min 28s\n",
      "Wall time: 4h 53min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffa2825a5f8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cvae.fit(x = [y_train_angle, x_train_poisson],\n",
    "         y = y_train_angle,\n",
    "         batch_size=1000,\n",
    "         epochs=300,\n",
    "         verbose=1,\n",
    "         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "375c59fd-2439-4154-9d5b-2a8ecec2a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './CVAE/DNN_angleTran_30g_{}'\n",
    "index = 1\n",
    "while os.path.isdir(path.format(index)):\n",
    "    index += 1\n",
    "path = path.format(index)\n",
    "cvae.encoder1.save(path + \"/encoder_1_test_v2.h5\")\n",
    "cvae.encoder2.save(path + \"/encoder_2_test_v2.h5\")\n",
    "cvae.decoder.save(path + \"/decoder_test_v2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc849ce-19de-4092-973c-5ffa7aaf8885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
