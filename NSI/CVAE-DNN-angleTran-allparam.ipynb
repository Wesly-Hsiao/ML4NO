{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "parliamentary-triangle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten , Convolution2D, MaxPooling2D , Lambda, Conv2D, Activation,Concatenate, Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam , SGD , Adagrad\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers , initializers, activations\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump, load\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "import os \n",
    "import sys\n",
    "import time\n",
    "import importlib\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "importlib.reload(logging)\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "\n",
    "# limit GPU memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   # Restrict TensorFlow to only use the first GPU\n",
    "try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "    gpus[0],\n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10000)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "except RuntimeError as e:\n",
    "# Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5d453a0-049a-4ef7-a000-556737024b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load(\"./nsi_data/sample_nsi_regression_1e7_v1.npz\")\n",
    "data_all = np.column_stack([training_data['ve_dune'][:,:36], training_data['vu_dune'][:,:36], training_data['vebar_dune'][:,:36], training_data['vubar_dune'][:,:36]])\n",
    "\n",
    "target = np.column_stack([training_data[\"theta13\"], training_data[\"theta23\"],\n",
    "                          np.sin(training_data[\"delta\"]/180*np.pi), np.cos(training_data[\"delta\"]/180*np.pi),\n",
    "                         training_data[\"mumu\"], training_data[\"emu\"],\n",
    "                         training_data[\"etau\"]])\n",
    "\n",
    "x_train = data_all\n",
    "y_train = target\n",
    "x_train_poisson = np.random.poisson(x_train)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lucky-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "particular-freedom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_parameter_inputs (Input [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_spectrum_inputs (InputL [(None, 144)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_parameter_1 (Dense)       (None, 64)           512         encoder_parameter_inputs[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_1 (Dense)        (None, 64)           9280        encoder_spectrum_inputs[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_parameter_2 (Dense)       (None, 32)           2080        dense_parameter_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_2 (Dense)        (None, 32)           2080        dense_spectrum_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_parameter_3 (Dense)       (None, 16)           528         dense_parameter_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_3 (Dense)        (None, 16)           528         dense_spectrum_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32)           0           dense_parameter_3[0][0]          \n",
      "                                                                 dense_spectrum_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 7)            231         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 7)            231         concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 15,470\n",
      "Trainable params: 15,470\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 7\n",
    "\n",
    "\"\"\"\n",
    "Encoder 1 (parameter + spectrum)\n",
    "\"\"\"\n",
    "encoder_parameter_inputs = layers.Input(shape=(len(y_train[0]),),name = 'encoder_parameter_inputs')\n",
    "x_parameter = layers.Dense(64, activation=\"relu\", name = 'dense_parameter_1')(encoder_parameter_inputs)\n",
    "x_parameter = layers.Dense(32, activation=\"relu\", name = 'dense_parameter_2')(x_parameter)\n",
    "x_parameter = layers.Dense(16, activation=\"relu\", name = 'dense_parameter_3')(x_parameter)\n",
    "\n",
    "encoder_spectrum_inputs = layers.Input(shape=(144),name = 'encoder_spectrum_inputs')\n",
    "x_spectrum = layers.Dense(64, activation=\"relu\", name = 'dense_spectrum_1')(encoder_spectrum_inputs)\n",
    "x_spectrum = layers.Dense(32, activation=\"relu\", name = 'dense_spectrum_2')(x_spectrum)\n",
    "x_spectrum = layers.Dense(16, activation=\"relu\", name = 'dense_spectrum_3')(x_spectrum)\n",
    "\n",
    "mergedOut_Encoder_1 = Concatenate()([x_parameter,x_spectrum])\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(mergedOut_Encoder_1)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(mergedOut_Encoder_1)\n",
    "\n",
    "encoder_1 = keras.Model([encoder_parameter_inputs, encoder_spectrum_inputs], [z_mean, z_log_var], name=\"encoder_1\")\n",
    "encoder_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "controlled-photographer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_spectrum_inputs (InputL [(None, 144)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_1 (Dense)        (None, 64)           9280        encoder_spectrum_inputs[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_2 (Dense)        (None, 32)           2080        dense_spectrum_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_3 (Dense)        (None, 16)           528         dense_spectrum_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 280)          4760        dense_spectrum_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 280)          4760        dense_spectrum_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "z_weight (Dense)                (None, 40)           680         dense_spectrum_3[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 22,088\n",
      "Trainable params: 22,088\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Encoder 2 (spectrum)\n",
    "\"\"\"\n",
    "encoder_spectrum_inputs = layers.Input(shape=(len(x_train[0]),),name = 'encoder_spectrum_inputs')\n",
    "x_spectrum = layers.Dense(64, activation=\"relu\", name = 'dense_spectrum_1')(encoder_spectrum_inputs)\n",
    "x_spectrum = layers.Dense(32, activation=\"relu\", name = 'dense_spectrum_2')(x_spectrum)\n",
    "x_spectrum = layers.Dense(16, activation=\"relu\", name = 'dense_spectrum_3')(x_spectrum)\n",
    "\n",
    "guassian_number = 40\n",
    "z_mean = layers.Dense(guassian_number*latent_dim, name=\"z_mean\")(x_spectrum)\n",
    "z_log_var = layers.Dense(guassian_number*latent_dim, name=\"z_log_var\")(x_spectrum)\n",
    "z_weight = layers.Dense(guassian_number, name=\"z_weight\")(x_spectrum)\n",
    "\n",
    "encoder_2 = keras.Model(encoder_spectrum_inputs, [z_mean, z_log_var, z_weight], name=\"encoder_2\")\n",
    "encoder_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fantastic-terrorism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_latent_inputs (InputLay [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_spectrum_inputs (InputL [(None, 144)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           512         decoder_latent_inputs[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_1 (Dense)        (None, 64)           9280        decoder_spectrum_inputs[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_2 (Dense)        (None, 32)           2080        dense_spectrum_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           528         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_spectrum_3 (Dense)        (None, 16)           528         dense_spectrum_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32)           0           dense_3[0][0]                    \n",
      "                                                                 dense_spectrum_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 7)            231         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 7)            231         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 15,470\n",
      "Trainable params: 15,470\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Decoder Model (latent + spectrum)\n",
    "\"\"\"\n",
    "latent_dim_2 = len(y_train[0])\n",
    "\n",
    "decoder_latent_inputs = keras.Input(shape=(latent_dim,),name = 'decoder_latent_inputs')\n",
    "x_latent = layers.Dense(64, activation=\"relu\", name = 'dense_1')(decoder_latent_inputs)\n",
    "x_latent = layers.Dense(32, activation=\"relu\", name = 'dense_2')(x_latent)\n",
    "x_latent = layers.Dense(16, activation=\"relu\", name = 'dense_3')(x_latent)\n",
    "\n",
    "# spectrum\n",
    "decoder_spectrum_inputs = layers.Input(shape=(144,),name = 'decoder_spectrum_inputs')\n",
    "x_spectrum = layers.Dense(64, activation=\"relu\", name = 'dense_spectrum_1')(decoder_spectrum_inputs)\n",
    "x_spectrum = layers.Dense(32, activation=\"relu\", name = 'dense_spectrum_2')(x_spectrum)\n",
    "x_spectrum = layers.Dense(16, activation=\"relu\", name = 'dense_spectrum_3')(x_spectrum)\n",
    "\n",
    "mergedOut_Decoder = Concatenate()([x_latent,x_spectrum])\n",
    "\n",
    "z2_mean = layers.Dense(latent_dim_2, name=\"z_mean\")(mergedOut_Decoder)\n",
    "z2_log_var = layers.Dense(latent_dim_2, name=\"z_log_var\")(mergedOut_Decoder)\n",
    "\n",
    "decoder = keras.Model([decoder_latent_inputs, decoder_spectrum_inputs], [z2_mean, z2_log_var], name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "specific-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(keras.Model):\n",
    "    def __init__(self, encoder1, encoder2, decoder, **kwargs):\n",
    "        super(CVAE, self).__init__(**kwargs)\n",
    "        self.encoder1 = encoder1\n",
    "        self.encoder2 = encoder2\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "                ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            SMALL_CONSTANT = 1e-12\n",
    "            \n",
    "            z1_mean, z1_log_var = self.encoder1(x)\n",
    "            \n",
    "            temp_var_q = SMALL_CONSTANT + tf.exp(z1_log_var)\n",
    "            mvn_q = tfp.distributions.MultivariateNormalDiag(\n",
    "                          loc=z1_mean,\n",
    "                          scale_diag=tf.sqrt(temp_var_q))\n",
    "            \n",
    "            z1 = mvn_q.sample()\n",
    "            \n",
    "            z2_mean, z2_log_var, z2_weight = self.encoder2(x[1])\n",
    "\n",
    "            z2_mean = tf.reshape(z2_mean, (-1, guassian_number, latent_dim))\n",
    "            z2_log_var = tf.reshape(z2_log_var, (-1, guassian_number, latent_dim))\n",
    "            z2_weight = tf.reshape(z2_weight, (-1, guassian_number))\n",
    "\n",
    "            temp_var_r1 = SMALL_CONSTANT + tf.exp(z2_log_var)\n",
    "            bimix_gauss = tfp.distributions.MixtureSameFamily(\n",
    "                          mixture_distribution=tfp.distributions.Categorical(logits=z2_weight),\n",
    "                          components_distribution=tfp.distributions.MultivariateNormalDiag(\n",
    "                          loc=z2_mean,\n",
    "                          scale_diag=tf.sqrt(temp_var_r1)))\n",
    "            \n",
    "            z2 = bimix_gauss.sample()\n",
    "            \n",
    "            reconstruction_mean, reconstruction_var = self.decoder([z1, x[1]])     \n",
    "            \n",
    "            temp_var_r2 = SMALL_CONSTANT + tf.exp(reconstruction_var)\n",
    "            reconstruction_parameter = tfp.distributions.MultivariateNormalDiag(\n",
    "                                     loc=reconstruction_mean,\n",
    "                                     scale_diag= tf.sqrt(temp_var_r2))\n",
    "            \n",
    "            r2 = reconstruction_parameter.sample()\n",
    "\n",
    "            log_q_q = mvn_q.log_prob(z1)\n",
    "            log_r1_q = bimix_gauss.log_prob(z1)\n",
    "            kl_loss = tf.reduce_mean(log_q_q - log_r1_q)\n",
    "            \n",
    "            reconstruction_parameter_loss = reconstruction_parameter.log_prob(y)\n",
    "            reconstruction_loss = -1.0*tf.reduce_mean(reconstruction_parameter_loss)\n",
    "            \n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "subsequent-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Building\n",
    "\"\"\"\n",
    "cvae = CVAE(encoder_1,encoder_2,decoder)\n",
    "cvae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-throw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/distributions/distribution.py:298: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it with `scale_diag` directly instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/linalg/linear_operator_diag.py:175: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n",
      "10000/10000 [==============================] - 67s 7ms/step - loss: 15.6887 - reconstruction_loss: 6.7993 - kl_loss: 0.2865\n",
      "Epoch 2/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: 3.8832 - reconstruction_loss: 3.6113 - kl_loss: 0.0613\n",
      "Epoch 3/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 3.2472 - reconstruction_loss: 2.7928 - kl_loss: 0.3307\n",
      "Epoch 4/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 2.6749 - reconstruction_loss: 1.2148 - kl_loss: 1.3168\n",
      "Epoch 5/300\n",
      "10000/10000 [==============================] - 67s 7ms/step - loss: 2.1750 - reconstruction_loss: 0.5354 - kl_loss: 1.5617\n",
      "Epoch 6/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 1.9076 - reconstruction_loss: 0.2311 - kl_loss: 1.6164\n",
      "Epoch 7/300\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 1.7147 - reconstruction_loss: 0.0283 - kl_loss: 1.6399\n",
      "Epoch 8/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 1.5238 - reconstruction_loss: -0.1893 - kl_loss: 1.6734\n",
      "Epoch 9/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 1.4162 - reconstruction_loss: -0.2717 - kl_loss: 1.6710\n",
      "Epoch 10/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 1.3519 - reconstruction_loss: -0.3007 - kl_loss: 1.6557\n",
      "Epoch 11/300\n",
      "10000/10000 [==============================] - 67s 7ms/step - loss: 1.2746 - reconstruction_loss: -0.3649 - kl_loss: 1.6628\n",
      "Epoch 12/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 1.1769 - reconstruction_loss: -0.5000 - kl_loss: 1.6648\n",
      "Epoch 13/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 1.1175 - reconstruction_loss: -0.5584 - kl_loss: 1.6635\n",
      "Epoch 14/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 1.0578 - reconstruction_loss: -0.6161 - kl_loss: 1.6656\n",
      "Epoch 15/300\n",
      "10000/10000 [==============================] - 67s 7ms/step - loss: 1.0036 - reconstruction_loss: -0.6706 - kl_loss: 1.6677\n",
      "Epoch 16/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.9789 - reconstruction_loss: -0.7126 - kl_loss: 1.6713\n",
      "Epoch 17/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: 0.9343 - reconstruction_loss: -0.7444 - kl_loss: 1.6603\n",
      "Epoch 18/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.8924 - reconstruction_loss: -0.7776 - kl_loss: 1.6608\n",
      "Epoch 19/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.8348 - reconstruction_loss: -0.8322 - kl_loss: 1.6673\n",
      "Epoch 20/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.7980 - reconstruction_loss: -0.8539 - kl_loss: 1.6568\n",
      "Epoch 21/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: 0.7757 - reconstruction_loss: -0.8835 - kl_loss: 1.6617\n",
      "Epoch 22/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: 0.7400 - reconstruction_loss: -0.9400 - kl_loss: 1.6740\n",
      "Epoch 23/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.7245 - reconstruction_loss: -0.9287 - kl_loss: 1.6749\n",
      "Epoch 24/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.6995 - reconstruction_loss: -1.0142 - kl_loss: 1.6927\n",
      "Epoch 25/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.6659 - reconstruction_loss: -1.0230 - kl_loss: 1.6798\n",
      "Epoch 26/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: 0.6459 - reconstruction_loss: -1.0631 - kl_loss: 1.6896\n",
      "Epoch 27/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: 0.6598 - reconstruction_loss: -1.0278 - kl_loss: 1.6743\n",
      "Epoch 28/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.6085 - reconstruction_loss: -1.0425 - kl_loss: 1.6799\n",
      "Epoch 29/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: 0.5612 - reconstruction_loss: -1.1242 - kl_loss: 1.6864\n",
      "Epoch 30/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: 0.5426 - reconstruction_loss: -1.1444 - kl_loss: 1.6861\n",
      "Epoch 31/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: 0.5227 - reconstruction_loss: -1.1587 - kl_loss: 1.6944\n",
      "Epoch 32/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: 0.5291 - reconstruction_loss: -1.1656 - kl_loss: 1.7009\n",
      "Epoch 33/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: 0.5049 - reconstruction_loss: -1.1884 - kl_loss: 1.6842\n",
      "Epoch 34/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.4800 - reconstruction_loss: -1.2139 - kl_loss: 1.6948\n",
      "Epoch 36/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.4479 - reconstruction_loss: -1.2197 - kl_loss: 1.6822\n",
      "Epoch 37/300\n",
      " 7243/10000 [====================>.........] - ETA: 18s - loss: 0.4317 - reconstruction_loss: -1.2676 - kl_loss: 1.7101"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.3746 - reconstruction_loss: -1.3187 - kl_loss: 1.6807\n",
      "Epoch 45/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: 0.3432 - reconstruction_loss: -1.3625 - kl_loss: 1.6990\n",
      "Epoch 46/300\n",
      " 8345/10000 [========================>.....] - ETA: 10s - loss: 0.3386 - reconstruction_loss: -1.3627 - kl_loss: 1.6952"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.2998 - reconstruction_loss: -1.3897 - kl_loss: 1.6921\n",
      "Epoch 53/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: 0.4021 - reconstruction_loss: -1.3771 - kl_loss: 1.7037\n",
      "Epoch 54/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.2401 - reconstruction_loss: -1.4583 - kl_loss: 1.6964\n",
      "Epoch 55/300\n",
      " 4199/10000 [===========>..................] - ETA: 38s - loss: 0.2174 - reconstruction_loss: -1.4853 - kl_loss: 1.7044"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 64s 6ms/step - loss: 0.1608 - reconstruction_loss: -1.5474 - kl_loss: 1.7037\n",
      "Epoch 63/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: 0.1522 - reconstruction_loss: -1.5615 - kl_loss: 1.7110\n",
      "Epoch 64/300\n",
      " 5275/10000 [==============>...............] - ETA: 31s - loss: 0.1529 - reconstruction_loss: -1.5839 - kl_loss: 1.7219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 65s 7ms/step - loss: 0.1088 - reconstruction_loss: -1.5632 - kl_loss: 1.6938\n",
      "Epoch 71/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.0997 - reconstruction_loss: -1.5956 - kl_loss: 1.6974\n",
      "Epoch 72/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.0902 - reconstruction_loss: -1.6018 - kl_loss: 1.6968\n",
      "Epoch 73/300\n",
      " 2013/10000 [=====>........................] - ETA: 51s - loss: 0.0786 - reconstruction_loss: -1.6071 - kl_loss: 1.6955"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.0575 - reconstruction_loss: -1.6455 - kl_loss: 1.7030\n",
      "Epoch 80/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: 0.0490 - reconstruction_loss: -1.6465 - kl_loss: 1.6964\n",
      "Epoch 81/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: 0.0685 - reconstruction_loss: -1.6587 - kl_loss: 1.7026\n",
      "Epoch 82/300\n",
      " 3472/10000 [=========>....................] - ETA: 43s - loss: 0.0272 - reconstruction_loss: -1.6611 - kl_loss: 1.7002"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 65s 7ms/step - loss: 0.0245 - reconstruction_loss: -1.6780 - kl_loss: 1.7035\n",
      "Epoch 89/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: 0.0133 - reconstruction_loss: -1.6944 - kl_loss: 1.7068\n",
      "Epoch 90/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: 0.0112 - reconstruction_loss: -1.6913 - kl_loss: 1.7063\n",
      "Epoch 91/300\n",
      " 3452/10000 [=========>....................] - ETA: 41s - loss: 0.0389 - reconstruction_loss: -1.6973 - kl_loss: 1.7034"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.0056 - reconstruction_loss: -1.6976 - kl_loss: 1.6998\n",
      "Epoch 93/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.0025 - reconstruction_loss: -1.7006 - kl_loss: 1.7136\n",
      "Epoch 94/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.0133 - reconstruction_loss: -1.6974 - kl_loss: 1.7095\n",
      "Epoch 95/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -4.1920e-04 - reconstruction_loss: -1.6946 - kl_loss: 1.6909\n",
      "Epoch 96/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: 0.0043 - reconstruction_loss: -1.6909 - kl_loss: 1.6919\n",
      "Epoch 97/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.0077 - reconstruction_loss: -1.6987 - kl_loss: 1.7004\n",
      "Epoch 98/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.0185 - reconstruction_loss: -1.7299 - kl_loss: 1.7224\n",
      "Epoch 99/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: 0.0224 - reconstruction_loss: -1.7218 - kl_loss: 1.7136\n",
      "Epoch 100/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.0220 - reconstruction_loss: -1.7177 - kl_loss: 1.7080\n",
      "Epoch 101/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.0062 - reconstruction_loss: -1.7084 - kl_loss: 1.6981\n",
      "Epoch 102/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.0066 - reconstruction_loss: -1.7216 - kl_loss: 1.7154\n",
      "Epoch 103/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.0265 - reconstruction_loss: -1.7345 - kl_loss: 1.7127\n",
      "Epoch 104/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.0213 - reconstruction_loss: -1.7354 - kl_loss: 1.7136\n",
      "Epoch 105/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.0264 - reconstruction_loss: -1.7277 - kl_loss: 1.7025\n",
      "Epoch 106/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.0400 - reconstruction_loss: -1.7427 - kl_loss: 1.7131\n",
      "Epoch 107/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.0199 - reconstruction_loss: -1.7404 - kl_loss: 1.7136\n",
      "Epoch 108/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.0212 - reconstruction_loss: -1.7193 - kl_loss: 1.7136\n",
      "Epoch 109/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.0276 - reconstruction_loss: -1.7291 - kl_loss: 1.7006\n",
      "Epoch 110/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.0380 - reconstruction_loss: -1.7403 - kl_loss: 1.7083\n",
      "Epoch 111/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.0496 - reconstruction_loss: -1.7712 - kl_loss: 1.7259\n",
      "Epoch 112/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.0496 - reconstruction_loss: -1.7422 - kl_loss: 1.7021\n",
      "Epoch 113/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.0505 - reconstruction_loss: -1.7540 - kl_loss: 1.7132\n",
      "Epoch 114/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.0506 - reconstruction_loss: -1.7683 - kl_loss: 1.7219\n",
      "Epoch 115/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.0548 - reconstruction_loss: -1.7652 - kl_loss: 1.7140\n",
      "Epoch 116/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.0485 - reconstruction_loss: -1.7605 - kl_loss: 1.7090\n",
      "Epoch 117/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.0489 - reconstruction_loss: -1.7378 - kl_loss: 1.6927\n",
      "Epoch 118/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.0357 - reconstruction_loss: -1.7504 - kl_loss: 1.7117\n",
      "Epoch 119/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.0578 - reconstruction_loss: -1.7672 - kl_loss: 1.7099\n",
      "Epoch 120/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.0585 - reconstruction_loss: -1.7860 - kl_loss: 1.7223\n",
      "Epoch 121/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.0614 - reconstruction_loss: -1.7811 - kl_loss: 1.7179\n",
      "Epoch 122/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.0559 - reconstruction_loss: -1.7813 - kl_loss: 1.7190\n",
      "Epoch 123/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.0673 - reconstruction_loss: -1.7795 - kl_loss: 1.7132\n",
      "Epoch 124/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.0658 - reconstruction_loss: -1.7841 - kl_loss: 1.7147\n",
      "Epoch 125/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.0679 - reconstruction_loss: -1.7929 - kl_loss: 1.7277\n",
      "Epoch 126/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.0826 - reconstruction_loss: -1.7958 - kl_loss: 1.7121\n",
      "Epoch 132/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.0928 - reconstruction_loss: -1.7943 - kl_loss: 1.7096\n",
      "Epoch 133/300\n",
      " 9383/10000 [===========================>..] - ETA: 4s - loss: -0.1057 - reconstruction_loss: -1.8300 - kl_loss: 1.7308"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.1126 - reconstruction_loss: -1.8445 - kl_loss: 1.7303\n",
      "Epoch 141/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.1126 - reconstruction_loss: -1.8315 - kl_loss: 1.7153\n",
      "Epoch 142/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.1231 - reconstruction_loss: -1.8496 - kl_loss: 1.7270\n",
      "Epoch 144/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.1264 - reconstruction_loss: -1.8598 - kl_loss: 1.7285\n",
      "Epoch 147/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.1361 - reconstruction_loss: -1.8579 - kl_loss: 1.7255\n",
      "Epoch 148/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.1447 - reconstruction_loss: -1.8649 - kl_loss: 1.7304\n",
      "Epoch 149/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.1443 - reconstruction_loss: -1.8834 - kl_loss: 1.7424\n",
      "Epoch 150/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.1491 - reconstruction_loss: -1.8867 - kl_loss: 1.7444\n",
      "Epoch 151/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.1362 - reconstruction_loss: -1.8848 - kl_loss: 1.7436\n",
      "Epoch 152/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.1503 - reconstruction_loss: -1.8749 - kl_loss: 1.7284\n",
      "Epoch 153/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.1451 - reconstruction_loss: -1.8801 - kl_loss: 1.7409\n",
      "Epoch 154/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.1250 - reconstruction_loss: -1.8706 - kl_loss: 1.7365\n",
      "Epoch 155/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.1620 - reconstruction_loss: -1.8957 - kl_loss: 1.7357\n",
      "Epoch 156/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.1530 - reconstruction_loss: -1.9008 - kl_loss: 1.7455\n",
      "Epoch 157/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.1505 - reconstruction_loss: -1.8730 - kl_loss: 1.7273\n",
      "Epoch 158/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.1727 - reconstruction_loss: -1.9056 - kl_loss: 1.7426\n",
      "Epoch 159/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.1504 - reconstruction_loss: -1.8982 - kl_loss: 1.7371\n",
      "Epoch 160/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.1683 - reconstruction_loss: -1.9054 - kl_loss: 1.7389\n",
      "Epoch 161/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.1697 - reconstruction_loss: -1.8989 - kl_loss: 1.7328\n",
      "Epoch 162/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.1773 - reconstruction_loss: -1.9307 - kl_loss: 1.7542\n",
      "Epoch 163/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.1857 - reconstruction_loss: -1.9165 - kl_loss: 1.7422\n",
      "Epoch 164/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.1753 - reconstruction_loss: -1.9206 - kl_loss: 1.7458\n",
      "Epoch 165/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.1793 - reconstruction_loss: -1.9212 - kl_loss: 1.7429\n",
      "Epoch 166/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.1811 - reconstruction_loss: -1.9187 - kl_loss: 1.7412\n",
      "Epoch 167/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.1905 - reconstruction_loss: -1.9270 - kl_loss: 1.7423\n",
      "Epoch 168/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.0290 - reconstruction_loss: -1.4736 - kl_loss: 1.7262\n",
      "Epoch 169/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: 0.0498 - reconstruction_loss: -1.7426 - kl_loss: 1.7525\n",
      "Epoch 170/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.0732 - reconstruction_loss: -1.8367 - kl_loss: 1.7576\n",
      "Epoch 171/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.1146 - reconstruction_loss: -1.8547 - kl_loss: 1.7373\n",
      "Epoch 172/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.1461 - reconstruction_loss: -1.8748 - kl_loss: 1.7346\n",
      "Epoch 173/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.1554 - reconstruction_loss: -1.8908 - kl_loss: 1.7331\n",
      "Epoch 174/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.1694 - reconstruction_loss: -1.9183 - kl_loss: 1.7454\n",
      "Epoch 175/300\n",
      "10000/10000 [==============================] - 67s 7ms/step - loss: -0.1743 - reconstruction_loss: -1.9153 - kl_loss: 1.7385\n",
      "Epoch 176/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.1913 - reconstruction_loss: -1.9473 - kl_loss: 1.7561\n",
      "Epoch 177/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.1897 - reconstruction_loss: -1.9385 - kl_loss: 1.7432\n",
      "Epoch 178/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.2020 - reconstruction_loss: -1.9493 - kl_loss: 1.7476\n",
      "Epoch 179/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.2117 - reconstruction_loss: -1.9578 - kl_loss: 1.7511\n",
      "Epoch 180/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.2032 - reconstruction_loss: -1.9748 - kl_loss: 1.7633\n",
      "Epoch 181/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.2224 - reconstruction_loss: -1.9694 - kl_loss: 1.7516\n",
      "Epoch 182/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.2145 - reconstruction_loss: -1.9552 - kl_loss: 1.7402\n",
      "Epoch 183/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.2244 - reconstruction_loss: -1.9719 - kl_loss: 1.7502\n",
      "Epoch 184/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.2262 - reconstruction_loss: -1.9712 - kl_loss: 1.7477\n",
      "Epoch 185/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.2328 - reconstruction_loss: -2.0041 - kl_loss: 1.7704\n",
      "Epoch 186/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.2317 - reconstruction_loss: -1.9916 - kl_loss: 1.7598\n",
      "Epoch 187/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.2394 - reconstruction_loss: -1.9842 - kl_loss: 1.7529\n",
      "Epoch 188/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.2452 - reconstruction_loss: -2.0081 - kl_loss: 1.7620\n",
      "Epoch 189/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.2566 - reconstruction_loss: -2.0027 - kl_loss: 1.7520\n",
      "Epoch 190/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.2548 - reconstruction_loss: -2.0070 - kl_loss: 1.7531\n",
      "Epoch 191/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.2432 - reconstruction_loss: -2.0030 - kl_loss: 1.7422\n",
      "Epoch 192/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.2662 - reconstruction_loss: -2.0300 - kl_loss: 1.7658\n",
      "Epoch 193/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.2708 - reconstruction_loss: -2.0104 - kl_loss: 1.7453\n",
      "Epoch 194/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.2629 - reconstruction_loss: -2.0249 - kl_loss: 1.7519\n",
      "Epoch 195/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.2710 - reconstruction_loss: -2.0291 - kl_loss: 1.7545\n",
      "Epoch 196/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.2733 - reconstruction_loss: -2.0393 - kl_loss: 1.7615\n",
      "Epoch 197/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.2774 - reconstruction_loss: -2.0350 - kl_loss: 1.7588\n",
      "Epoch 198/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.2824 - reconstruction_loss: -2.0449 - kl_loss: 1.7616\n",
      "Epoch 199/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.2908 - reconstruction_loss: -2.0335 - kl_loss: 1.7652\n",
      "Epoch 200/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.2882 - reconstruction_loss: -2.0451 - kl_loss: 1.7584\n",
      "Epoch 201/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.2848 - reconstruction_loss: -2.0345 - kl_loss: 1.7482\n",
      "Epoch 202/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.2949 - reconstruction_loss: -2.0428 - kl_loss: 1.7536\n",
      "Epoch 203/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.2921 - reconstruction_loss: -2.0389 - kl_loss: 1.7498\n",
      "Epoch 204/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.2874 - reconstruction_loss: -2.0353 - kl_loss: 1.7534\n",
      "Epoch 205/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.2958 - reconstruction_loss: -2.0534 - kl_loss: 1.7587\n",
      "Epoch 206/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.2991 - reconstruction_loss: -2.0754 - kl_loss: 1.7701\n",
      "Epoch 207/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.3035 - reconstruction_loss: -2.0604 - kl_loss: 1.7546\n",
      "Epoch 208/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.3166 - reconstruction_loss: -2.0706 - kl_loss: 1.7591\n",
      "Epoch 209/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.3172 - reconstruction_loss: -2.0742 - kl_loss: 1.7641\n",
      "Epoch 210/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.3213 - reconstruction_loss: -2.0815 - kl_loss: 1.7624\n",
      "Epoch 211/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.3103 - reconstruction_loss: -2.0717 - kl_loss: 1.7565\n",
      "Epoch 212/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.3166 - reconstruction_loss: -2.0692 - kl_loss: 1.7469\n",
      "Epoch 213/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.3099 - reconstruction_loss: -2.0605 - kl_loss: 1.7582\n",
      "Epoch 214/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.3284 - reconstruction_loss: -2.0939 - kl_loss: 1.7664\n",
      "Epoch 215/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.3314 - reconstruction_loss: -2.0862 - kl_loss: 1.7583\n",
      "Epoch 216/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.3193 - reconstruction_loss: -2.0843 - kl_loss: 1.7587\n",
      "Epoch 217/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.3402 - reconstruction_loss: -2.0998 - kl_loss: 1.7628\n",
      "Epoch 218/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.3513 - reconstruction_loss: -2.1110 - kl_loss: 1.7653\n",
      "Epoch 219/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.3409 - reconstruction_loss: -2.0929 - kl_loss: 1.7522\n",
      "Epoch 220/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.3464 - reconstruction_loss: -2.1000 - kl_loss: 1.7548\n",
      "Epoch 221/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.3431 - reconstruction_loss: -2.0998 - kl_loss: 1.7520\n",
      "Epoch 222/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.3400 - reconstruction_loss: -2.1217 - kl_loss: 1.7705\n",
      "Epoch 223/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.3514 - reconstruction_loss: -2.1040 - kl_loss: 1.7581\n",
      "Epoch 224/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.3546 - reconstruction_loss: -2.1045 - kl_loss: 1.7465\n",
      "Epoch 225/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.3689 - reconstruction_loss: -2.1264 - kl_loss: 1.7637\n",
      "Epoch 226/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.3606 - reconstruction_loss: -2.1084 - kl_loss: 1.7523\n",
      "Epoch 227/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.3718 - reconstruction_loss: -2.1362 - kl_loss: 1.7674\n",
      "Epoch 228/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.3621 - reconstruction_loss: -2.1094 - kl_loss: 1.7438\n",
      "Epoch 229/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.3731 - reconstruction_loss: -2.1279 - kl_loss: 1.7589\n",
      "Epoch 230/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.3761 - reconstruction_loss: -2.1241 - kl_loss: 1.7523\n",
      "Epoch 231/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.3806 - reconstruction_loss: -2.1355 - kl_loss: 1.7591\n",
      "Epoch 232/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.3902 - reconstruction_loss: -2.1663 - kl_loss: 1.7795\n",
      "Epoch 233/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.3915 - reconstruction_loss: -2.1549 - kl_loss: 1.7702\n",
      "Epoch 234/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.3974 - reconstruction_loss: -2.1466 - kl_loss: 1.7577\n",
      "Epoch 235/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.3940 - reconstruction_loss: -2.1617 - kl_loss: 1.7693\n",
      "Epoch 236/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.3950 - reconstruction_loss: -2.1469 - kl_loss: 1.7531\n",
      "Epoch 237/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.4064 - reconstruction_loss: -2.1656 - kl_loss: 1.7641\n",
      "Epoch 238/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.4089 - reconstruction_loss: -2.1777 - kl_loss: 1.7710\n",
      "Epoch 239/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.3990 - reconstruction_loss: -2.1759 - kl_loss: 1.7667\n",
      "Epoch 240/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.4055 - reconstruction_loss: -2.1653 - kl_loss: 1.7577\n",
      "Epoch 241/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.4158 - reconstruction_loss: -2.1659 - kl_loss: 1.7510\n",
      "Epoch 242/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.4171 - reconstruction_loss: -2.1802 - kl_loss: 1.7628\n",
      "Epoch 243/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.4239 - reconstruction_loss: -2.1955 - kl_loss: 1.7712\n",
      "Epoch 244/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.4088 - reconstruction_loss: -2.1761 - kl_loss: 1.7670\n",
      "Epoch 245/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.4283 - reconstruction_loss: -2.1935 - kl_loss: 1.7664\n",
      "Epoch 246/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.4330 - reconstruction_loss: -2.1902 - kl_loss: 1.7625\n",
      "Epoch 247/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.4302 - reconstruction_loss: -2.1786 - kl_loss: 1.7496\n",
      "Epoch 248/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.4384 - reconstruction_loss: -2.2069 - kl_loss: 1.7662\n",
      "Epoch 249/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.4413 - reconstruction_loss: -2.2106 - kl_loss: 1.7700\n",
      "Epoch 250/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.4511 - reconstruction_loss: -2.2050 - kl_loss: 1.7693\n",
      "Epoch 251/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.4533 - reconstruction_loss: -2.2259 - kl_loss: 1.7718\n",
      "Epoch 252/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.4471 - reconstruction_loss: -2.2057 - kl_loss: 1.7586\n",
      "Epoch 253/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.4531 - reconstruction_loss: -2.2163 - kl_loss: 1.7644\n",
      "Epoch 254/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.4654 - reconstruction_loss: -2.2667 - kl_loss: 1.8042\n",
      "Epoch 255/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.4645 - reconstruction_loss: -2.2445 - kl_loss: 1.7808\n",
      "Epoch 256/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.4567 - reconstruction_loss: -2.2352 - kl_loss: 1.7719\n",
      "Epoch 257/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.4662 - reconstruction_loss: -2.2149 - kl_loss: 1.7575\n",
      "Epoch 258/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.4637 - reconstruction_loss: -2.2541 - kl_loss: 1.7847\n",
      "Epoch 259/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.4639 - reconstruction_loss: -2.2267 - kl_loss: 1.7620\n",
      "Epoch 260/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.4520 - reconstruction_loss: -2.2274 - kl_loss: 1.7608\n",
      "Epoch 261/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.4699 - reconstruction_loss: -2.2239 - kl_loss: 1.7566\n",
      "Epoch 262/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.4813 - reconstruction_loss: -2.2384 - kl_loss: 1.7631\n",
      "Epoch 263/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.4748 - reconstruction_loss: -2.2463 - kl_loss: 1.7829\n",
      "Epoch 264/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.4822 - reconstruction_loss: -2.2376 - kl_loss: 1.7609\n",
      "Epoch 265/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.4911 - reconstruction_loss: -2.2702 - kl_loss: 1.7853\n",
      "Epoch 266/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.4849 - reconstruction_loss: -2.2471 - kl_loss: 1.7695\n",
      "Epoch 267/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.4812 - reconstruction_loss: -2.2451 - kl_loss: 1.7628\n",
      "Epoch 268/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.4845 - reconstruction_loss: -2.2639 - kl_loss: 1.7779\n",
      "Epoch 269/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.4919 - reconstruction_loss: -2.2356 - kl_loss: 1.7552\n",
      "Epoch 270/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.5002 - reconstruction_loss: -2.2756 - kl_loss: 1.7811\n",
      "Epoch 271/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.4974 - reconstruction_loss: -2.2596 - kl_loss: 1.7679\n",
      "Epoch 272/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.5047 - reconstruction_loss: -2.2666 - kl_loss: 1.7683\n",
      "Epoch 273/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.5016 - reconstruction_loss: -2.2762 - kl_loss: 1.7765\n",
      "Epoch 274/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.4760 - reconstruction_loss: -2.2696 - kl_loss: 1.7854\n",
      "Epoch 275/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.5010 - reconstruction_loss: -2.2747 - kl_loss: 1.7704\n",
      "Epoch 276/300\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: -0.5075 - reconstruction_loss: -2.2912 - kl_loss: 1.7836\n",
      "Epoch 277/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.4891 - reconstruction_loss: -2.2663 - kl_loss: 1.7761\n",
      "Epoch 278/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.5107 - reconstruction_loss: -2.2801 - kl_loss: 1.7752\n",
      "Epoch 279/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.5134 - reconstruction_loss: -2.2982 - kl_loss: 1.7855\n",
      "Epoch 280/300\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: -0.5092 - reconstruction_loss: -2.2795 - kl_loss: 1.7670\n",
      "Epoch 281/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.5087 - reconstruction_loss: -2.2937 - kl_loss: 1.7840\n",
      "Epoch 282/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.5243 - reconstruction_loss: -2.2974 - kl_loss: 1.7806\n",
      "Epoch 283/300\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: -0.5271 - reconstruction_loss: -2.3123 - kl_loss: 1.7935\n",
      "Epoch 284/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.5054 - reconstruction_loss: -2.2861 - kl_loss: 1.7829\n",
      "Epoch 285/300\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: -0.5193 - reconstruction_loss: -2.2934 - kl_loss: 1.7721\n",
      "Epoch 288/300\n",
      " 2496/10000 [======>.......................] - ETA: 48s - loss: -0.5293 - reconstruction_loss: -2.3322 - kl_loss: 1.8010"
     ]
    }
   ],
   "source": [
    "for i in range(2, 11):\n",
    "    training_data = np.load(\"./nsi_data/sample_nsi_regression_1e7_v{}.npz\".format(i))\n",
    "    data_all = np.column_stack([training_data['ve_dune'][:,:36], training_data['vu_dune'][:,:36], training_data['vebar_dune'][:,:36], training_data['vubar_dune'][:,:36]])\n",
    "\n",
    "    target = np.column_stack([training_data[\"theta13\"], training_data[\"theta23\"],\n",
    "                              np.sin(training_data[\"delta\"]/180*np.pi), np.cos(training_data[\"delta\"]/180*np.pi),\n",
    "                             training_data[\"mumu\"], training_data[\"emu\"],\n",
    "                             training_data[\"etau\"]])\n",
    "\n",
    "    x_train = data_all\n",
    "    y_train = target\n",
    "    x_train_poisson = np.random.poisson(x_train)/1000\n",
    "    cvae.fit(x = [y_train, x_train_poisson],\n",
    "             y = y_train,\n",
    "             batch_size=1000,\n",
    "             epochs=300,\n",
    "             verbose=1,\n",
    "             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c59fd-2439-4154-9d5b-2a8ecec2a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save file\n",
    "\"\"\"\n",
    "path = './CVAE/DNN_angleTran_allparam_{},{}'.format(guassian_number, latent_dim) + '_{}'\n",
    "index = 1\n",
    "while os.path.isdir(path.format(index)):\n",
    "    index += 1\n",
    "path = path.format(index)\n",
    "cvae.encoder1.save(path + \"/encoder_1.h5\")\n",
    "cvae.encoder2.save(path + \"/encoder_2.h5\")\n",
    "cvae.decoder.save(path + \"/decoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160072e-d51d-44ca-be18-b1680848543a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
